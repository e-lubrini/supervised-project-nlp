@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@misc{wang2020generalizing,
      title={Generalizing from a Few Examples: A Survey on Few-Shot Learning}, 
      author={Yaqing Wang and Quanming Yao and James Kwok and Lionel M. Ni},
      year={2020},
      eprint={1904.05046},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{caldas2019leaf,
      title={LEAF: A Benchmark for Federated Settings}, 
      author={Sebastian Caldas and Sai Meher Karthik Duddu and Peter Wu and Tian Li and Jakub Konečný and H. Brendan McMahan and Virginia Smith and Ameet Talwalkar},
      year={2019},
      eprint={1812.01097},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{yu2018diverse,
      title={Diverse Few-Shot Text Classification with Multiple Metrics}, 
      author={Mo Yu and Xiaoxiao Guo and Jinfeng Yi and Shiyu Chang and Saloni Potdar and Yu Cheng and Gerald Tesauro and Haoyu Wang and Bowen Zhou},
      year={2018},
      eprint={1805.07513},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{geng2019induction,
      title={Induction Networks for Few-Shot Text Classification}, 
      author={Ruiying Geng and Binhua Li and Yongbin Li and Xiaodan Zhu and Ping Jian and Jian Sun},
      year={2019},
      eprint={1902.10482},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{NIPS2016_90e13578,
 author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and kavukcuoglu, koray and Wierstra, Daan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Matching Networks for One Shot Learning},
 url = {https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf},
 volume = {29},
 year = {2016}
}


@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@misc{lewis2019bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{wolpert,
title = "Stacked generalization",
journal = "Neural Networks",
volume = "5",
number = "2",
pages = "241 - 259",
year = "1992",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(05)80023-1",
url = "http://www.sciencedirect.com/science/article/pii/S0893608005800231",
author = "David H. Wolpert",
keywords = "Generalization and induction, Combining generalizers, Learning set preprocessing, cross-validation, Error estimation and correction",
abstract = "This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory."
}

@inproceedings{zero,
author={Lampert, Christoph H. and Nickisch, Hannes and Harmeling, Stefan},  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},   title={Learning to detect unseen object classes by between-class attribute transfer},   year={2009},  volume={},  number={},  pages={951-958},  doi={10.1109/CVPR.2009.5206594}}