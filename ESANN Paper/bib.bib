@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@misc{wang2020generalizing,
      title={Generalizing from a Few Examples: A Survey on Few-Shot Learning}, 
      author={Yaqing Wang and Quanming Yao and James Kwok and Lionel M. Ni},
      year={2020},
      eprint={1904.05046},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{caldas2019leaf,
      title={LEAF: A Benchmark for Federated Settings}, 
      author={Sebastian Caldas and Sai Meher Karthik Duddu and Peter Wu and Tian Li and Jakub Konečný and H. Brendan McMahan and Virginia Smith and Ameet Talwalkar},
      year={2019},
      eprint={1812.01097},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{yu2018diverse,
      title={Diverse Few-Shot Text Classification with Multiple Metrics}, 
      author={Mo Yu and Xiaoxiao Guo and Jinfeng Yi and Shiyu Chang and Saloni Potdar and Yu Cheng and Gerald Tesauro and Haoyu Wang and Bowen Zhou},
      year={2018},
      eprint={1805.07513},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{geng-etal-2019-induction,
    title = "Induction Networks for Few-Shot Text Classification",
    author = "Geng, Ruiying  and
      Li, Binhua  and
      Li, Yongbin  and
      Zhu, Xiaodan  and
      Jian, Ping  and
      Sun, Jian",
    booktitle = "Proceedings of the 2019 EMNLP-IJCNLP",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1403",
    doi = "10.18653/v1/D19-1403",
    pages = "3904--3913",
    abstract = "Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes. In such challenging scenarios, recent studies have used meta-learning to simulate the few-shot task, in which new queries are compared to a small support set at the sample-wise level. However, this sample-wise comparison may be severely disturbed by the various expressions in the same class. Therefore, we should be able to learn a general representation of each class in the support set and then compare it to new queries. In this paper, we propose a novel Induction Network to learn such a generalized class-wise representation, by innovatively leveraging the dynamic routing algorithm in meta-learning. In this way, we find the model is able to induce and generalize better. We evaluate the proposed model on a well-studied sentiment classification dataset (English) and a real-world dialogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, proving the effectiveness of class-wise generalization in few-shot text classification.",
}


@inproceedings{NIPS2016_90e13578,
 author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and kavukcuoglu, koray and Wierstra, Daan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Matching Networks for One Shot Learning},
 url = {https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf},
 volume = {29},
 year = {2016}
}


@misc{lewis2019bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{wolpert,
title = "Stacked generalization",
journal = "Neural Networks",
volume = "5",
number = "2",
pages = "241 - 259",
year = "1992",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(05)80023-1",
url = "http://www.sciencedirect.com/science/article/pii/S0893608005800231",
author = "David H. Wolpert",
keywords = "Generalization and induction, Combining generalizers, Learning set preprocessing, cross-validation, Error estimation and correction",
abstract = "This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory."
}

@inproceedings{zero,
author={Lampert, Christoph H. and Nickisch, Hannes and Harmeling, Stefan},  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},   title={Learning to detect unseen object classes by between-class attribute transfer},   year={2009},  volume={},  number={},  pages={951-958},  doi={10.1109/CVPR.2009.5206594}}

@article{problems,
url = {http://dx.doi.org/10.1561/2200000083},
year = {2021},
volume = {14},
journal = {Foundations and Trends in Machine Learning},
title = {Advances and Open Problems in Federated Learning},
doi = {10.1561/2200000083},
issn = {1935-8237},
number = {1},
pages = {-},
author = {Edited by: Peter Kairouz and H. Brendan McMahan}
}

@inproceedings{sabour2017dynamic,
  title={Dynamic routing between capsules},
  author={Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={3859--3869},
  year={2017}
}

@inproceedings{socher2013reasoning,
  title={Reasoning with neural tensor networks for knowledge base completion},
  author={Socher, Richard and Chen, Danqi and Manning, Christopher D and Ng, Andrew},
  booktitle={Advances in neural information processing systems},
  pages={926--934},
  year={2013},
  organization={Citeseer}
}


@inproceedings{mishra2018simple,
  title={A Simple Neural Attentive Meta-Learner},
  author={Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{sung2018learning,
  title={Learning to compare: Relation network for few-shot learning},
  author={Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip HS and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1199--1208},
  year={2018}
}

@inproceedings{bart,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020}
}

@article{lu2021pretrained,
  title={Pretrained transformers as universal computation engines},
  author={Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  journal={arXiv preprint arXiv:2103.05247},
  year={2021}
}